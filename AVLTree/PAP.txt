We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates theprobabilitythatasamplecamefromthetrainingdataratherthan G. Thetraining procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are deﬁned by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.Table 3 provides the comparative results of generalized FSL on the miniImageNet dataset. We can observe that: 1) Our approach achieves the best results on all evaluation metrics, with bigger margins than those under the standard setting. This shows that our model has the strongest generalization ability under this more challenging setting. 2) Our approach outperforms the PN and RN, because we learn global class representation for each class, while they estimate episodic class representations. 3) MN yields much lower results than our approaches. It is expected: context embedding encodes examples of all classes; with so many base class examples, they overwhelm those in the novel classes, making context embedding fail to emphasize novel class features. Our sample synthesis strategy increases intra-class variance and thus alleviates the data scarcity issue in novel classes.from imbalanced data sets is a relatively new challenge for many of today’s data mining applications. From applications in Web mining to text categorization to biomedical data analysis [1], this challenge manifests itself in two common forms: minority interests and rare instances. Minority interests arise in domains where rare objects (minority class samples) are of great interest, and it is the objective of the machine learning algorithm to identify these minority class examples as accurately as possible. For instance, in ﬁnancial engineering, it is important to detect fraudulent credit card activities in a pool of large transactions [2] [3]. Rare instances, on the other hand, concerns itself with situations where data representing a particular event is limited compared to other distributions [4] [5], such as the detection of oil spills from satellite images [6]. One should note that many imbalanced learning problems are caused by a combination of these two factors. For instance, in biomedical data analysis, the data samples for different kinds of cancers are normally very limited (rare instances) compared to normal non-cancerous cases; therefore, the ratio of the minority class to the majority class can be signiﬁcant (at a ratio of 1 to 1000 or even more [4][7][8]). On the other hand, it is essential to predict the presence of cancers, or further classify different types of cancers as accurate as possible for earlier and proper treatment .There exists a great potential for predictive maintenance (PdM), since the number of sensor-equipped devices and the need for eﬀective maintenance strategies is growing. However, we have discovered a mismatch between the PdM performance criteria and the business requirements. Traditional optimization criteria, such as the F1-score, favor PdM models that correctly forecast a high number of failures, but they usually neglect the economic cost associated with true/false positive/negatives. We propose to closely examine the business processes in order to gain a better understanding of the cost structure and incorporate the individual cost factors into the PdM optimization. An application-speciﬁc cost function has been introduced as well as compared to traditional performance measures. Our evaluation has demonstrated that the proposed cost function is able to achieve signiﬁcantly higher savings and furthermore prevents ﬁnancial loss caused by inaccurate predictions on low quality data.n addition we have presented a general recipe for integrating various business objectives into an economic cost function, which is achieved by assigning weights to the individual components of a confusion matrix. Moreover, our study demonstrates that it is possible to design cost functions that incorporate general PdM objectives, such as a long transition and short prediction window. We believe that our proposed recipe has the potential to provide even better PdM solutions in many application area. In the near future we will deploy our solution in a production environment and extent it to diﬀerent PdM use cases. Furthermore, we will investigate additional cost factors, such as the cost of diﬀerent service personal involved in the maintenance process. Another interesting and untouched aspect is the inﬂuence of the sliding window step size and the potential of overlapping prediction windows.

