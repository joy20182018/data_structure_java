We propose a new framework for estimating
 generative models via an adversarial process,
  in which we simultaneously train two models:
  a generative model G that captures the data
  distribution, and a discriminative model D
  that estimates theprobabilitythatasamplecame
  fromthetrainingdataratherthan G. Thetraining
  procedure for G is to maximize the probability
   of D making a mistake. This framework corresp
   onds to a minimax two-player game. In the spac
   e of arbitrary functions G and D, a unique solu
   tion exists, with G recovering the training data
    distribution and D equal to 1 2 everywhere. In t
    he case where G and D are deﬁned by multilayer p
    erceptrons, the entire system can be trained with
     backpropagation. There is no need for any M
     arkov chains or unrolled approximate infere
     nce networks during either training or gener
     ation of samples. Experiments demonstrate t
     he potential of the framework through quali
     tative and quantitative evaluation of the g
     enerated samples.Table 3 provides the compa
     rative results of generalized FSL on the
     miniImageNet dataset. We can observe that: 1
     ) Our approach achieves the best results on a
     ll evaluation metrics, with bigger margins th
     an those under the standard setting. This sho
     ws that our model has the strongest generaliz
     ation ability under this more challenging set
     ting. 2) Our approach outperforms the PN and
     RN, because we learn global class represent
     ation for each class, while they estimate ep
     isodic class representations. 3) MN yields mu
     ch lower results than our approaches. It is e
     xpected: context embedding encodes examples o
     f all classes; with so many base class exampl
     es, they overwhelm those in the novel classes
     , making context embedding fail to emphasize
     novel class features. Our sample synthesis
      increases intra-class variance and thus alle
      viates the data scarcity issue in novel
      .from imbalanced data sets is a relatively new
      challenge for many of today’s data mining appl
      ications. From applications in Web mining to text categorization to biomedical data analysis [1], this challenge manifests itself in two common forms: minority interests and rare instances. Minority interests arise in domains where rare objects (minority class samples) are of great interest, and it is the objective of the machine learning algorithm to identify these minority class examples as accurately as possible. For instance, in ﬁnancial engineering, it is important to detect fraudulent credit card activities in a pool of large transactions [2] [3]. Rare instances, on the other hand, concerns itself with situations where data representing a particular event is limited compared to other distributions [4] [5], such as the detection of oil spills from satellite images [6]. One should note that many imbalanced learning problems are caused by a combination of these two factors. For instance, in biomedical data analysis, the data samples for different kinds of cancers are normally very limited (rare instances) compared to normal non-cancerous cases; therefore, the ratio of the minority class to the majority class can be signiﬁcant (at a ratio of 1 to 1000 or even more [4][7][8]). On the other hand, it is essential to predict the presence of cancers, or further classify different types of cancers as accurate as possible for earlier and proper treatment .There exists a great potential for predictive maintenance (PdM), since the number of sensor-equipped devices and the need for eﬀective maintenance strategies is growing. However, we have discovered a mismatch between the PdM performance criteria and the business requirements. Traditional optimization criteria, such as the F1-score, favor PdM models that correctly forecast a high number of failures, but they usually neglect the economic cost associated with true/false positive/negatives. We propose to closely examine the business processes in order to gain a better understanding of the cost structure and incorporate the individual cost factors into the PdM optimization. An application-speciﬁc cost function has been introduced as well as compared to traditional performance measures. Our evaluation has demonstrated that the proposed cost function is able to achieve signiﬁcantly higher savings and furthermore prevents ﬁnancial loss caused by inaccurate predictions on low quality data.n addition we have presented a general recipe for integrating various business objectives into an economic cost function, which is achieved by assigning weights to the individual components of a confusion matrix. Moreover, our study demonstrates that it is possible to design cost functions that incorporate general PdM objectives, such as a long transition and short prediction window. We believe that our proposed recipe has the potential to provide even better PdM solutions in many application area. In the near future we will deploy our solution in a production environment and extent it to diﬀerent PdM use cases. Furthermore, we will investigate additional cost factors, such as the cost of diﬀerent service personal involved in the maintenance process. Another interesting and untouched aspect is the inﬂuence of the sliding window step size and the potential of overlapping prediction windows.


